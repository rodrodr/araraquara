---
title: "Dia 4 - Análise sintática e classificação"
author: "Rodrigo Rodrigues-Silveira"
lang: pt-BR
format: 
  html:
    code-link: true
    code-fold: true
    code-overflow: wrap
    code-summary: "Código"
    code-annotations: below
---

## Análise sintática

Muitas vezes, a análise de palavras soltas não é suficiente para entender o seu significado. Por exemplo, "estrela" pode significar tanto um astro celeste como uma pessoa que destaca no seu âmbito profissional. Também pode ser um termo despectivo: "esse cara é muito estrela". Por essa razão, muitas vezes poder ser útil empregar algumas técnicas que ajudem a determinar o contexto e reducir os espaços de ambigüidades.

A análise sintática corresponde ao conjunto de técnicas que ajudam a situar as palavras no seu contexto. Está formada por tres técnicas: *Parts of Speech (POS) Tagging*, o reconhecimento do entidades (*Named Entity Recognition - NER*) e a análise de dependencias (*dependency parsing*). Aqui consideraremos as duas primeiras. 

O primeiro passo é anotar o texto. Para isso necessitamos utilizar algum modelo previamente treinado para atribuir etiquetas às palavras. Utilizaremos aqui o pacote udpipe do R, que contém modelos aceitavelmente precisos para fazer as análises.

**Paso 1: Carrega o corpus de discursos presidenciais**

```{r, warning=F, message=F, error=F}

# Carrega os pacotes básicos
library(readtext)
library(quanteda)
library(stringi)

# Carrega os textos
tx <- readtext("../../Data/Discursos_Presidentes/")

# Cria o corpus
cp <- corpus(tx)

# Documenta
docvars(cp, "Presidente") <- c("Deodoro da Fonseca","Floriano Peixoto",
                               "Prudente de Moraes","Campos Sales",
                               "Rodrigues Alvez","Affonso Penna",
                               "Hermes da Fonseca", "Wenceslau Bras",
                               "Epitacio Pessoa", "Arthur Bernardes",
                               "Washingtonn Luis","Getulio Vargas",
                               "Getulio Vargas","Getulio Vargas",
                               "Eurico Gaspar Dutra","Getulio Vargas",
                               "Juscelino Kubitschek", "Janio Quadros",
                               "Joao Goulart","Castelo Branco",
                               "Costa e Silva","Emilio Garrastazu Medici",
                               "Ernesto Geisel", "Joao Batista Figueiredo",
                               "Jose Sarney", "Tancredo Neves",
                               "Fernando Collor de Melo", "Itamar Franco",
                               "Fernando Henrique Cardoso","Fernando Henrique Cardoso",
                               "Luis Inacio Lula da Silva", "Luis Inacio Lula da Silva",
                               "Dilma Rousseff","Dilma Rousseff",
                               "Michel Temer", "Jair Bolsonaro",
                               "Luis Inacio Lula da Silva")

docvars(cp, "Ano") <- as.numeric(substr(tx$doc_id,1,4))

ce <- corpus_group(cp, Presidente)

```

**Paso 2: Anota o corpus usando o UDPIPE**

```{r, warning=F, message=F, error=F}

# Abre o pacote udpipe para realizar 
# o etiquetado dos textos
library(udpipe)

# Descarrega o modelo previamente treinado
m_pt   <- udpipe_download_model(language = "portuguese")

# Carrega o modelo
m_pt <- udpipe_load_model(m_pt)

# Gera as anotações e etiquetas
d <- udpipe_annotate(m_pt, 
                     x=ce, 
                     doc_id = docnames(ce)) %>%
  as.data.frame() %>%
  dplyr::select(-sentence)

# Visualiza os resultados
reactable::reactable(d[,c("doc_id",
                          "sentence_id",
                          "token_id",
                          "token",
                          "lemma",
                          "upos",
                          "feats")], 
                     resizable = T, 
                     filterable = T)

```



### POS tagging


```{r, warning=F, message=F, error=F, fig.height=8}

# Cria uma lista com os presidentes
# que se desejam visualizar
presidente<- c("Luis Inacio Lula da Silva","Fernando Henrique Cardoso")

# Cria uma lista para guardar os
# gráficos de cada presidente e não
# ter que repetir código
gr <- list()

# para cada presidente na lista
for(i in 1:length(presidente)){
  
  # Seleciona nomes, verbos e adjetivos 
  # da base que foi etiquetada no
  # passo anterior
  adj <- d[which(
            d$upos%in%c("NOUN",
                        "VERB",
                        "ADJ") 
            & d$doc_id==presidente[i]),]

  # Calcula a frequência de cada palavra
  # por tipo
  ag <- aggregate(list(freq=adj$lemma),
                by=list(Tipo=adj$upos,
                        lemma=adj$lemma),
                length)

  # Seleciona os 20 mais frequentes
  # de cada tipo
  ag <- ag |> 
    dplyr::slice_max(
                freq,
                n=20, 
                by=Tipo, 
                with_ties = F)

  # Altera as descrições para
  # facilitar o entendimento do
  # gráfico
  ag$Tipo[ag$Tipo=="ADJ"] <- "Adjetivo"
  ag$Tipo[ag$Tipo=="NOUN"] <- "Nombre"
  ag$Tipo[ag$Tipo=="VERB"] <- "Verbo"

  # Cria o gráfico de barras
  # para cada partido e tipo 
  # gramatical
  library(ggplot2)
  library(forcats)


  p <- ggplot(
    ag, aes(x=freq, y=fct_reorder(lemma, freq), fill=Tipo))+
    geom_bar(stat="identity")+
    theme_minimal()+
    theme(panel.grid = element_blank(),
          plot.title = ggtext::element_markdown(),
          legend.position = "none")+
    labs(title=paste0("**",presidente[i],"**"))+
    ylab("")+xlab("Menciones")+
    facet_wrap(~Tipo, scales = "free", )

  # Salva o gráfico de cada partido
  # na lista para sua posterior
  # visualização em um painel comum
  gr[[i]] <- p

}

# Organiza os dois gráficos como um
library(grid)
library(gridExtra)

grid.arrange(gr[[1]], gr[[2]])

```





```{r}

# Função selPOSTags - seleciona duas etiquetas que
# aparecem uma depois da outra em uma frase
selPOSTags <- function(data, 
                       tag_first, 
                       tag_second, 
                       docid=NULL, 
                       source="lemma",
                       tag.var="upos",
                       exclude.dyads=TRUE,
                       n.words=NULL){

  # retira os acentos e converte em
  # minúsculas os tokens ou lemas 
  data[[source]] <- tolower(
                      stringi::stri_trans_general(
                        data[[source]], 
                        "ascii"))
  
  data$tagvar <- data[[tag.var]]
  
  # Unifica as etiquetas para filtrar
  # os dados
  tags <- c(tag_first,tag_second)
  
  # filta os dados
  dx <- data[data$tagvar%in%tags,]
  
  # Se se define um texto ou documento,
  # seleciona só os elementos desse
  # documento 
  if(!is.null(docid)){
    dx <- dx[dx$doc_id==docid,]
  }
  
  # gera colunas para:
  # etiqueta imediatamente posterior
  # termo imediatamente posterior
  # número da frase do termo 
  # imediatamente posterior
  dx$pos_next <- c(dx$tagvar[2:nrow(dx)],NA)
  dx$term_next <- c(dx[[source]][2:nrow(dx)],NA)
  dx$sen_next <- c(dx$sentence_id[2:nrow(dx)],NA)
  
  # Mantém somente combinações que se 
  # encontram em uma mesma frase
  dx <- dx[dx$sentence_id==dx$sen_next,]
  
  # Seleciona as combinações de 
  # etiquetas (first=ADJ e second=NOUN, 
  # por exemplo). Pode-se utilizar mais
  # de um tag de cada lado (isso aumenta 
  # as configurações possíveis e torna
  # a análise mais complexa).
  dx <- dx[
            dx$tagvar%in%tag_first & 
            dx$pos_next%in%tag_second,]
  
  # Cria um contador para verificar a
  # frequência e agrega pela combinação
  # entre termos  
  dx$count <- 1
  ag <- aggregate(list(value=dx$count), 
                  by=list(from=dx[[source]],
                          to=dx$term_next),
                  sum)

  # Conta o número de vezes que cada
  # termo aparece. Isto é vital
  # para eliminar as díadas isoladas
  d1 <- data.frame(table(ag$from))
  names(d1) <- c("from","ffrm")
  d2 <- data.frame(table(ag$to))
  names(d2) <- c("to","fto")
  ag <- merge(ag, d1, by="from")
  ag <- merge(ag, d2, by="to")

  # Define quantas vezes aparecem as
  # duas palavras juntas. Se menos de 
  # duas, significa que são díadas
  # isoladas
  ag$order <- (ag$ffrm+ag$fto/2)
  
  # Se queremos exclui-las, aquí: 
  if(exclude.dyads==TRUE){
    ag <- ag[ag$order>2,]
  }
  
  # Ordena de acordo com a frequencia
  # de forma descendente
  ag <- ag[order(ag$order,
                 ag$value, 
                 decreasing = T),]
  
  # Se não se estabelece um número
  # máximo de palavras a retornar,
  # seleciona todas.
  if(is.null(n.words)){
    n.words <- nrow(ag)
  }else if(n.words>nrow(ag)){
    n.words <- nrow(ag)
  }
  
  # Realiza la seleção
  ag <- ag[1:n.words,
           c("from", "to","value")]

  # Retorna os resultados
  # para o usuário
  return(ag)
}


```


```{r}

# Gera as anotações e etiquetas
d <- udpipe_annotate(m_pt, 
                     x=ce, 
                     doc_id = docnames(ce)) %>%
  as.data.frame() %>%
  dplyr::select(-sentence)


# Elimina algumas que são comuns
# demais e, nesse caso, não ajudam
# muito a entender os padrões
d <- d[! d$lemma%in%c("grande",
                      "ter",
                      "fazer",
                      "dar",
                      "ser",
                      "dizer"),]

# Aplica a função que seleciona
# as etiquetas, nesse caso verbos
# seguiddos de substantivos
ag <- selPOSTags(d, 
                 n.words = 100,
                 tag_first = c("VERB"), 
                 tag_second = "NOUN")

# Visualizamos os resultados
library(tenet)

plotChord(ag)

```


## Named Entity Recognition


Aqui utilizaremos duas formas de extrair as entidades.

Forma 1: com modelos pré-treinados

```{r}

library(spacyr)

# Se o modelo não estiver instalado,
# instalá-lo (só uma vez) com o comando
# abaixo
# spacy_install(lang_models = "pt_core_news_lg")

# Carrega o modelo
spacy_initialize(model = "pt_core_news_lg")

# Anota o corpus
prs <- spacy_parse(cp)

# Finaliza o modelo
spacy_finalize()

# Elimina as palavras que não 
# correspondem a entidades
dx <- prs[prs$entity!="",]

# Seleciona só entidades identificadas
# como pessoas
da <- dx[dx$entity%in%c("PER_B","PER_I") 
         & dx$pos=="PROPN",]

# Visualiza os resultados
reactable::reactable(da, 
          resizable = T, 
          filterable = T)

```


Forma 2: com expressões regulares

```{r}

# Expressão regular que seleciona os seguintes padroes:
# 1) Palavra iniciada por maiúscula seguida por espaco e palavra com minúscula, logo seguida de uma ou mais com maiúscula: Paulo de Tarso Mendes, por exemplo.
# 2) Palavras todas iniciadas por maiúsculas: Companhia Siderúrgica Nacional. 
pat <- "(([A-Z][a-zA-Z]+)(\\s)([a-z]+)(?=\\s[A-Z])(?:\\s[A-Z][a-zA-Z]+)+)|(([A-Z][a-z]+)(?=\\s[A-Z])(?:\\s[A-Z][a-zA-Z]+)+)"

# Extrai os valores do texto
kw <- stri_extract_all(stri_trans_general(tx$text, "ascii"), 
                       regex = pat, simplify=T)

# Converte em data.frame
kw <- data.frame(kw)

# Associa o nome do documento
# aos resultados
kw$doc_id <- tx$doc_id

# Altera o formato de wide (amplo)
# para longo
library(reshape2)
kw <- melt(kw,id="doc_id")

# Elimina as variáveis e 
# valores desnecessários
kw$variable <- NULL
kw <- kw[kw$value!="" & ! is.na(kw$value),]

# Visualiza os resultados
reactable::reactable(kw, 
          resizable = T, 
          filterable = T)

```


## Classificação


### Análise de cluster



```{r, fig.height=8}

# Carrega os pacotes
library(quanteda)
library(quanteda.textstats)

# Cria os tokens e as
# matrizes de frequência de termos
tk <- tokens(cp, 
             remove_numbers = T,
             remove_symbols = T, 
             remove_punct = T)
df <- dfm(tk)

# Retira os stowords
df <- dfm_remove(df, stopwords("pt"))

# Calcula as distâncias
dis <- as.dist(textstat_dist(df))

# Faz a análise de cluster
clust <- hclust(dis)

# Visualiza os resultados
plot(clust, xlab = "Distance", ylab = NULL)

```


```{r}

# Faz a mesma coisa, mas com 
# um gráfico interativo
forceClusTree(cp, 
              maxRadius = 8,
              lang = "pt")

```


### Rainette


```{r, warning=F, message=F, error=F, fig.height=8}

# Carrega o pacote de classificação
# baseada no método de Reinert
library(rainette)

# Repete os passos básicos
tk <- tokens(cp, 
             remove_numbers = T, 
             remove_symbols = T, 
             remove_punct = T)

tk <- tokens_remove(tk, 
                    stopwords("pt"))

tf <- dfm(tk)

# Aplica o método 
rn <- rainette(tf, k = 20)

rainette_plot(rn, tf)


```


### LDA


```{r}


# Repete os passos anteriores
library(quanteda)

tk <- tokens(cp, 
             remove_punct = TRUE, 
             remove_numbers = TRUE, 
             remove_symbol = TRUE)

tk <- tokens_remove(tk, 
                    stopwords("pt"))

df <- dfm(tk) %>% 
        dfm_trim(
            min_termfreq = 0.8, 
            termfreq_type = "quantile",
            max_docfreq = 0.1, 
            docfreq_type = "prop")

# Carrega o modelo do LDA
library(topicmodels)

# Realiza o LDA
tp <- LDA(df, k = 5)

# Extrai os termos principais
# de cada categoria
tr <- terms(tp, k=10)

# Visualiza
library(reactable)
reactable(tr)

```



```{r}

# Agora extrai os tópicos que 
# predominan em cada documento
tpa <- topics(tp)

# Melhora a apresentação para
# facilitar a visualização
tpa <- data.frame(topico=tpa, documento=names(tpa))

row.names(tpa) <- 1:nrow(tpa)

reactable(tpa, resizable = T)

```



Probabilidades de uma palavra pertencer a um tópico:

```{r}

# Visualiza a probabilidade 
# de uma palavra pertencer a um
# tópico (estatística beta)
library(tidytext)

ap <- tidy(tp, matrix = "beta")

ap$beta <- round(ap$beta, 4)

ap <- ap[ap$beta>0,]

ap <- ap[order(ap$term, -ap$beta),]

reactable(ap, resizable = T)

```


Porcentagem do texto


```{r}

# Visualiza a porcentagem do
# documento correspondente a cada
# tópico (estatística gama)
ap <- tidy(tp, matrix = "gamma")

ap$gamma <- round(ap$gamma, 4)

ap <- ap[ap$gamma>0,]

ap <- ap[order(ap$topic, -ap$gamma),]

reactable(ap, resizable = T)


```


```{r}

# Gera um gráfico com o gamma
ggplot(ap, aes(y=document, x=gamma))+
  geom_boxplot()+
  theme_minimal()+
  theme(panel.grid = element_blank())

```

